{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression practical\n",
    "By Dominic Waithe 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "#This is a simple numpy implementation of the correlation function.\n",
    "def correlation(data1, data2):\n",
    "    \"data1 & data2 should be numpy arrays.\"\n",
    "    mean1 = data1.mean() \n",
    "    mean2 = data2.mean()\n",
    "    std1 = data1.std()\n",
    "    std2 = data2.std()\n",
    "    corr = ((data1*data2).mean()-mean1*mean2)/(std1*std2)\n",
    "    return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the analytical solution for solving simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make some data.\n",
    "data = np.array([[1,6],[2,5],[3,7],[4,7]])\n",
    "n = data.shape[0]\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "#Using the expanded equations from the notes.\n",
    "# equation (3).\n",
    "b0 = (np.average(y)*(np.sum(x**2))-np.average(x)*np.sum(x*y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b0',b0) \n",
    "# equation (2).\n",
    "b1 = ((np.sum(x*y))-n*np.average(x)*np.average(y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b1',b1)\n",
    "#visualisation\n",
    "xx = np.linspace(0,5,2)\n",
    "yy = np.array(b0+ b1 * xx)\n",
    "plot(xx,yy.T,color='b')\n",
    "scatter(data[:,0], data[:,1],color='r')\n",
    "\n",
    "#You could do it for higher dimension input data (i.e. 3D or nD data, but the equations get very long!!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the generalisable solution for solving linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(X'X) This is equivalent to the matrix [[n,np.sum(x)],[np.sum(x),np.sum(x**2)]]\n",
    "X = np.array([np.ones(n),data[:,0]]).T\n",
    "XtX = X.T.dot(X)\n",
    "#(X'Y) This is equivalent to the matrix [[np.sum(y)],[np.sum(x*y)]]\n",
    "y = np.array(data[:,1]).reshape(-1,1)\n",
    "XtY = X.T.dot(y)\n",
    "\n",
    "betaHat = np.linalg.solve(XtX,XtY)#This does the hard work! \n",
    "#Above can involve a lot of memory if the arrays are very long and high in dimension.\n",
    "print('b0',betaHat[0])\n",
    "print('b1',betaHat[1])\n",
    "#Visualisation.\n",
    "xx = np.linspace(0,5,2)\n",
    "yy = np.array(betaHat[0]+ betaHat[1] * xx)\n",
    "plot(xx,yy.T,color='b')\n",
    "scatter(data[:,0], data[:,1],color='r')\n",
    "#Should give the same answer as above :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO. \n",
    "#Download some data and fit it using the linear regression methods above.\n",
    "#e.g. sklearn.datasets.load_diabetes(return_X_y=False)\n",
    "#import sklearn.datasets\n",
    "#a =sklearn.datasets.load_diabetes(return_X_y=False)\n",
    "#Just plot two variables at a time.\n",
    "#calculate the correlation for each of the independent variables with respect to the dependent.\n",
    "#correlation(a['data'][:,num],a['target']\n",
    "#Find the most correlated and fit a line to it.\n",
    "#measure the correlation\n",
    "#Make a figure, label the axis, and give it a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#### This is the generalisable solution for solving linear regression.\n",
    "###In this case optimised for 3-D data.\n",
    "\n",
    "#We reformat our data a little bit. Notice where the extra has gone.\n",
    "x_nD = np.array([[1,2,3,4],[3,7,10,5]])\n",
    "y = np.array([6,5,7,7])\n",
    "n = data.shape[0]\n",
    "\n",
    "#(X'X) This is equivalent to the matrix [[n,np.sum(x)],[np.sum(x),np.sum(x**2)]]\n",
    "X = np.array([np.ones(n),x_nD[0],x_nD[1]]).T\n",
    "XtX = X.T.dot(X)\n",
    "#(X'Y) This is equivalent to the matrix [[np.sum(y)],[np.sum(x*y)]]\n",
    "y = np.array(y).reshape(-1,1)\n",
    "XtY = X.T.dot(y)\n",
    "betaHat = np.linalg.solve(XtX,XtY)#This does the hard work! \n",
    "#Above can involve a lot of memory if the arrays are very long and high in dimension.\n",
    "\n",
    "#print the paramters of our line.\n",
    "print('b0',betaHat[0])\n",
    "print('b1',betaHat[1])\n",
    "print('b2',betaHat[2])\n",
    "\n",
    "fig = figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax = Axes3D(fig)\n",
    "ax.scatter(x_nD[0], x_nD[1], y, zdir='z', s=20, c=None, depthshade=True)\n",
    "xs = np.linspace(0,5,2)\n",
    "ys = np.linspace(0,5,2)\n",
    "yy = np.array(betaHat[0]+ betaHat[1] * xs + betaHat[2]*ys)\n",
    "ax.plot(xs, ys,yy,'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "\n",
    "#Can you see how the above code could be changed to make it work in 4-D or more?\n",
    "#Try and create code which works for your diabetes data. \n",
    "#You won't be able to visualise the output as a graph.\n",
    "\n",
    "\n",
    "#print the parameters of your line.\n",
    "print('b0',betaHat[0])\n",
    "print('b1',betaHat[1])\n",
    "print('b2',betaHat[2])\n",
    "#print('b3',betaHat[3])\n",
    "#print('b4',betaHat[4])\n",
    "#print('b5',betaHat[5])\n",
    "#How can you check if your method is working correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent as an alternative optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import time\n",
    "#How about if we add lots of data now.\n",
    "x_nD = np.array(np.random.random((100000,2)))\n",
    "y = x_nD[:,0]*9.0+x_nD[:,1]\n",
    "SGDRegressor(loss=\"squared_loss\")\n",
    "clf = SGDRegressor()\n",
    "\n",
    "X0 = x_nD[:,0].reshape(1,-1)*5.0  # SGDRegressor is very particular on input \"X\" and insists on a true matrix \n",
    "y0 = y.tolist()\n",
    "\n",
    "t1 = time.time()\n",
    "clf.fit(X0.T,y0)#Stochastic Gradient Descent approach.\n",
    "t0 = time.time()\n",
    "print(t0-t1,\"ms SGD iterative method.\")\n",
    "\n",
    "n = x_nD.shape[0]\n",
    "\n",
    "t1 = time.time()\n",
    "#(X'X) This is equivalent to the matrix [[n,np.sum(x)],[np.sum(x),np.sum(x**2)]]\n",
    "X = np.array([np.ones(n),x_nD[:,0]*5.0]).T\n",
    "XtX = X.T.dot(X)\n",
    "#(X'Y) This is equivalent to the matrix [[np.sum(y)],[np.sum(x*y)]]\n",
    "y = np.array(y0).reshape(-1,1)\n",
    "XtY = X.T.dot(y)\n",
    "betaHat = np.linalg.solve(XtX,XtY)#Analytical linear algebra approach.\n",
    "t0 = time.time()\n",
    "print(t0-t1,\"ms analytical least squares method.\")\n",
    "#Above can involve a lot of memory if the arrays are very long and high in dimension.\n",
    "#At this size of data however, at low dimension it is very fast compared SGD.\n",
    "\n",
    "\n",
    "print('b0',clf.intercept_[0],'iterative')\n",
    "print('b1',clf.coef_[0],'iterative')\n",
    "print('b0',betaHat[0][0],'analytical')\n",
    "print('b1',betaHat[1][0],'analytical')\n",
    "\n",
    "\n",
    "plot(X0[0],y0,'o')\n",
    "xx = np.linspace(0,5,2)\n",
    "yy = np.array(clf.intercept_+ clf.coef_[0] * xx)\n",
    "plot(xx,yy.T,color='b')\n",
    "yy = np.array(betaHat[0]+ betaHat[1] * xx)\n",
    "plot(xx,yy.T,color='r')\n",
    "###Which is faster???\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###TODO. sklearn has a dedicated least squares solver for solving linear regression problems. \n",
    "###Find this function and test it on the above data and your diabetes data. Which method is fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

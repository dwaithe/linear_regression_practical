{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression practical\n",
    "By Dominic Waithe 2018. Updated (2019).\n",
    "\n",
    "This is a Jupyter notebook. Each cell represents a different python script. Variables which are assigned are accessible from all the cells. As as consequence of this, the order and the number of times a cell is run can be important as variables can be changed multiple times.\n",
    "\n",
    "- To run a cell, click the cell and press \"shift-enter\" on your keyboard. Alternatively, click the \">|\" button above or click \"Run\" from the \"Cell\" menu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please run this cell you will need it later.\n",
    "#It loads in the functions necessary for this practical.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#This is a simple numpy implementation of the correlation function.\n",
    "def correlation(data1, data2):\n",
    "    \"data1 & data2 should be numpy arrays.\"\n",
    "    mean1 = data1.mean() \n",
    "    mean2 = data2.mean()\n",
    "    std1 = data1.std()\n",
    "    std2 = data2.std()\n",
    "    corr = ((data1*data2).mean()-mean1*mean2)/(std1*std2)\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "#Here is some data. [[x0,y0],[x1,y1],[x2,y2],...,[xn,yn]]\n",
    "data = [[18,  510],[20,  590],[22,  560],[23,  510],[23,  460],[25,  490],[27,  560],[28,  510],\n",
    "[29,  460],[32,  410],[37,  420],[41,  460],[46,  450],[49,  380],[53,  460],[55,  420],[63,  350],\n",
    "[65,  420],[66,  300],[67,  410],[68,  300],[70,  390],[71,  320],[72,  370],[73,  280],[74,  420],\n",
    "[75,  460],[77,  360],[79,  310],[82,  360]]\n",
    "x = np.array(data)[:,0]\n",
    "y = np.array(data)[:,1]\n",
    "#How to plot data like this.\n",
    "plt.plot(x,y,'ko')\n",
    "#TODO: calculate the correlation value for this data. Would we apply linear regression on this data?\n",
    "print(np.corrcoef(x,y))\n",
    "corr = correlation(x,y)\n",
    "print('corr',corr)\n",
    "print('r2',corr**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the analytical solution for solving simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.array([[18,  510],[20,  590],[22,  560],[23,  510],[23,  460],[25,  490],[27,  560],[28,  510],\n",
    "[29,  460],[32,  410],[37,  420],[41,  460],[46,  450],[49,  380],[53,  460],[55,  420],[63,  350],\n",
    "[65,  420],[66,  300],[67,  410],[68,  300],[70,  390],[71,  320],[72,  370],[73,  280],[74,  420],\n",
    "[75,  460],[77,  360],[79,  310],[82,  360]])\n",
    "n = data.shape[0]\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "#Using the expanded equations from the notes.\n",
    "# equation (1).\n",
    "b0 = (np.average(y)*(np.sum(x**2))-np.average(x)*np.sum(x*y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b0',b0) \n",
    "# equation (2).\n",
    "b1 = ((np.sum(x*y))-n*np.average(x)*np.average(y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b1',b1)\n",
    "#visualisation\n",
    "xx = np.linspace(np.min(x),np.max(x),2)\n",
    "yy = np.array(b0+ b1 * xx)\n",
    "plt.plot(xx,yy,'-',color='pink')\n",
    "plt.scatter(data[:,0], data[:,1],color='k')\n",
    "plt.title(\"XY plot comparing the independent variable and the dependent variable \")\n",
    "plt.xlabel(\"Independent variable (x)\")\n",
    "plt.ylabel(\"Dependent variable (y)\")\n",
    "#You could do it for higher dimension input data (i.e. 3D or nD data, but the equations get very long!!!!).\n",
    "\n",
    "#TODO: Load the interactive demonstration:\n",
    "#http://userweb.molbiol.ox.ac.uk/dwaithe/WIMM_Advanced_Imaging/linearRegressiond4.html\n",
    "#Can you get the same parameters as the fit. Do you agree with the choice? \n",
    "#Bonus: Calculate the error. Is it the same as in the demonstration? Can you get lower. This is quite challenging so\n",
    "#you can skip this bonus challenge if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the generalisable solution for solving linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.array([[18,  510],[20,  590],[22,  560],[23,  510],[23,  460],[25,  490],[27,  560],[28,  510],\n",
    "[29,  460],[32,  410],[37,  420],[41,  460],[46,  450],[49,  380],[53,  460],[55,  420],[63,  350],\n",
    "[65,  420],[66,  300],[67,  410],[68,  300],[70,  390],[71,  320],[72,  370],[73,  280],[74,  420],\n",
    "[75,  460],[77,  360],[79,  310],[82,  360]])\n",
    "n = data.shape[0]\n",
    "#(X'X) This is equivalent to the matrix [[n,np.sum(x)],[np.sum(x),np.sum(x**2)]]\n",
    "X = np.array([np.ones(n),data[:,0]]).T\n",
    "XtX = X.T.dot(X) #left side of equation (3)\n",
    "#(X'Y) This is equivalent to the matrix [[np.sum(y)],[np.sum(x*y)]]\n",
    "y = np.array(data[:,1])\n",
    "XtY = X.T.dot(y) #right side of equation (4)\n",
    "\n",
    "#This is equation (4) the solve bit does the inversion. It does the hard work!\n",
    "beta = np.linalg.solve(XtX,XtY)\n",
    "#Above can involve a lot of memory if they are high in dimension.\n",
    "\n",
    "#TODO: print the output paramaters and visualise the data.\n",
    "#Should give the same answer as above :-).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO. \n",
    "#Download some data and fit it using the linear regression methods above.\n",
    "#e.g. sklearn.datasets.load_diabetes(return_X_y=False)\n",
    "#https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset\n",
    "import sklearn.datasets\n",
    "a = sklearn.datasets.load_diabetes(return_X_y=False)\n",
    "#Just plot two variables at a time.\n",
    "#calculate the correlation for each of the independent variables with respect to the dependent.\n",
    "for num in range(0,a['data'].shape[1]):\n",
    "    print(a['feature_names'][num],'vs dependent',correlation(a['data'][:,num],a['target']))\n",
    "#Find the most correlated and fit a line to it.\n",
    "#measure the correlation\n",
    "#Make a figure, label the axis, and give it a title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimension multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#### This is the generalisable solution for solving linear regression.\n",
    "###In this case optimised for 3-D data.\n",
    "\n",
    "#We reformat our data a little bit. Notice where the extra has gone.\n",
    "x_nD = np.array([[1,2,3,4,5,6],[3,7,10,5,4,3]])\n",
    "\n",
    "y = np.array([6,5,7,7,8,9.5])\n",
    "n = y.shape[0]\n",
    "\n",
    "#(X'X) Notice the extra column.\n",
    "X = np.array([np.ones(n),x_nD[0],x_nD[1]]).T\n",
    "XtX = X.T.dot(X)\n",
    "#(X'Y) \n",
    "y = np.array(y)\n",
    "XtY = X.T.dot(y)\n",
    "beta = np.linalg.solve(XtX,XtY)#This does the hard work! \n",
    "#Above can involve a lot of memory if they are high in dimension.\n",
    "\n",
    "#print the paramters of our line.\n",
    "print('b0',beta[0])\n",
    "print('b1',beta[1])\n",
    "print('b2',beta[2])\n",
    "\n",
    "\n",
    "\n",
    "#Visualise the plane which we fit in the case of two independent and one denpendent variables.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(x_nD[0], x_nD[1], y, zdir='z', s=20, c=None, depthshade=True)\n",
    "x = y = np.arange(np.min(x_nD), np.max(x_nD), 0.05)\n",
    "x0, x1 = np.meshgrid(x, y)\n",
    "Z = np.array(beta[0]+ beta[1] * x0 + beta[2]*x1)\n",
    "\n",
    "\n",
    "ax.plot_surface(x0,x1,Z,color='red')\n",
    "plt.title(\"XYZ plot comparing two independent variables and the dependent variable \")\n",
    "plt.xlabel(\"Independent variable (x0)\")\n",
    "plt.ylabel(\"Independent variable (x1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "\n",
    "#Can you see how the above code could be changed to make it work in 4-D or more?\n",
    "#Try and create code which works for dimensions of the  diabetes data. \n",
    "#You won't be able to visualise the output as a graph.\n",
    "\n",
    "\n",
    "#print the parameters of your line.\n",
    "print('b0',beta[0])\n",
    "print('b1',beta[1])\n",
    "print('b2',beta[2])\n",
    "#print('b3',beta[3])\n",
    "#print('b4',beta[4])\n",
    "#print('b5',beta[5])\n",
    "#How can you check if your method is working correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent as an alternative optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import time\n",
    "#How about if we add lots of data now.\n",
    "x_nD = np.array(np.random.random((100000,2)))\n",
    "y = x_nD[:,0]*9.0+x_nD[:,1]\n",
    "SGDRegressor(loss=\"squared_loss\")\n",
    "clf = SGDRegressor()\n",
    "\n",
    "X0 = x_nD[:,0].reshape(1,-1)*5.0  # SGDRegressor is very particular on input \"X\" and insists on a true matrix \n",
    "y0 = y.tolist()\n",
    "\n",
    "t1 = time.time()\n",
    "clf.fit(X0.T,y0)#Stochastic Gradient Descent approach.\n",
    "t0 = time.time()\n",
    "print(t0-t1,\"ms SGD iterative method.\")\n",
    "\n",
    "n = x_nD.shape[0]\n",
    "\n",
    "t1 = time.time()\n",
    "#(X'X) This is equivalent to the matrix [[n,np.sum(x)],[np.sum(x),np.sum(x**2)]]\n",
    "X = np.array([np.ones(n),x_nD[:,0]*5.0]).T\n",
    "XtX = X.T.dot(X)\n",
    "#(X'Y) This is equivalent to the matrix [[np.sum(y)],[np.sum(x*y)]]\n",
    "y = np.array(y0).reshape(-1,1)\n",
    "XtY = X.T.dot(y)\n",
    "beta = np.linalg.solve(XtX,XtY)#Analytical linear algebra approach.\n",
    "t0 = time.time()\n",
    "print(t0-t1,\"ms analytical least squares method.\")\n",
    "#Your previous analytical method can involve a lot of memory if the arrays are very long and high in dimension.\n",
    "#At this size of data however, at low dimension, it is very fast compared SGD.\n",
    "\n",
    "\n",
    "print('b0',clf.intercept_[0],'iterative')\n",
    "print('b1',clf.coef_[0],'iterative')\n",
    "print('b0',beta[0][0],'analytical')\n",
    "print('b1',beta[1][0],'analytical')\n",
    "\n",
    "\n",
    "plt.plot(X0[0],y0,'o')\n",
    "xx = np.linspace(0,5,2)\n",
    "yy = np.array(clf.intercept_+ clf.coef_[0] * xx)\n",
    "plt.plot(xx,yy.T,color='b')\n",
    "yy = np.array(beta[0]+ beta[1] * xx)\n",
    "plt.plot(xx,yy.T,color='r')\n",
    "###Which is faster???\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO. sklearn has a dedicated least squares solver for solving linear regression problems. \n",
    "###Find this function and test it on the above data and your diabetes data. Which method is fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyWa9zaxY5oI"
   },
   "source": [
    "# `Linear Regression on images Tutorial`\n",
    "by Dominic Waithe (c) 2020\n",
    "### Introduction\n",
    "* This notebook covers different code implementations from the scikit learn and pandas library, including:\n",
    "  * Importing image data\n",
    "  * Recaling and Standardizing Data \n",
    "  * Linear regression\n",
    "  * Residuals\n",
    "  * Plotting \n",
    "\n",
    "### Approach and Exercises\n",
    "* Each section will help reinforce your learning. We suggest you:\n",
    "    * Read each cell before you run it.\n",
    "    * Ponder the meaning and function of all the commands.\n",
    "    * Try altering some of the code to see what happens.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KA2bg4ruhi0G"
   },
   "source": [
    "## 1. Tutorial set up \n",
    "* Import statements\n",
    "* Loading the datasets\n",
    "* Reviewing the ML problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VrrYp-SY2Z4"
   },
   "outputs": [],
   "source": [
    "# import statements \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# we will generally import scikit learn libraries when the are required, so you understand which specifc libraries are required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files download, you should only need to do this once.\n",
    "import requests, zipfile, io\n",
    "zip_file_url = \"http://sara.molbiol.ox.ac.uk/dwaithe/files/fluoCells.zip\"\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "#It will download the data form the server, and unzip in the directory where you run your notebook from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK67KgsUDAvk"
   },
   "outputs": [],
   "source": [
    "fig,pane =plt.subplots(1,2,figsize=(15,15))\n",
    "img = plt.imread('fluoCells/001cell.png')#Reads the image in.\n",
    "gti = plt.imread('fluoCells/001dots.png')#Reads the ground-truth image in.\n",
    "pane[0].imshow(img)#plots image to left pane\n",
    "pane[1].imshow(gti)#plots image to right pane.\n",
    "pane[0].set_title('Grayscale image')\n",
    "pane[1].set_title('Ground-truth image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * We have a number of images (200) with matching ground-truth images.  \n",
    " * In future we want to be able to predict number of cells in image, without having to make ground-truth. \n",
    " * Can we learn the relationship between these images and their ground-truths and predict the cell counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the images and make a dataframe for the data\n",
    "name_array = []\n",
    "gray_intensity = []\n",
    "gt_count = []\n",
    "for i in range(0,200):\n",
    "    n = str(i+1).zfill(3)\n",
    "    name_array.append(n+\"cell\")\n",
    "    img = plt.imread(\"fluoCells/\"+n+\"cell.png\")#Reads the image in.\n",
    "    gti = plt.imread(\"fluoCells/\"+n+\"dots.png\")\n",
    "    gray_intensity.append(np.sum(img))\n",
    "    gt_count.append(np.sum(gti))\n",
    "\n",
    "#print and example\n",
    "data = pd.DataFrame(list(zip(gray_intensity,gt_count)),columns=['gray_intensity','ground-truth'],index=name_array)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing and visualisation\n",
    "* Scale data\n",
    "* Generate training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract our training pairs.\n",
    "y = data['ground-truth']\n",
    "X = np.array(data['gray_intensity']).reshape(-1, 1)#We reshape here to make our list into a 2-D array. This is need for the scalar function, which experts a 2-D array.\n",
    "\n",
    "# Rescaling and Standardizing Data \n",
    "# Machine Learning algorithms can receive perform benefits from data that has been rescaled and standardized, either in terms of speed or accuracy, or both.\n",
    "# There are a number of different ways of doing this, and implementations may vary because of algorithm requirements or personal preference.\n",
    "# This can be done easily using the pandas framework.\n",
    "# Create scaler object:\n",
    "scaler = StandardScaler()\n",
    "# fit the scaler - this calculates the mean and std of the data.\n",
    "scaler.fit(X)\n",
    "# transform the data using the fitted scaler - this applies the transform using the fit\n",
    "sta_X = scaler.transform(X)\n",
    "\n",
    "print('mean',np.round(np.mean(sta_X),0))\n",
    "print('std',np.std(sta_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets the plot the relationship between our X and y\n",
    "plt.plot(sta_X,y,'o')\n",
    "print('has a high degree of correlation',np.corrcoef(sta_X.reshape(-1),y)[1][0])\n",
    "#But is not perfect and so we can use linear regression, its not deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make predictions with our model and so we will split the data into indpendent train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(sta_X,y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting machine learning model\n",
    "* Fit data to model\n",
    "* Extract parameters\n",
    "* Visualise model fit\n",
    "* Calculate and visualise residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our sanity check. Our method should beat this.\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "y_prob = dummy_regr.predict(X_test)\n",
    "print('Dummy regressor:')\n",
    "print('Mean Squared Error (the lower the better)',mean_squared_error(y_test,y_prob))\n",
    "\n",
    "#This is our linear regression model.\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train,y_train)\n",
    "y_pred = linear.predict(X_test)\n",
    "print('Linear regressor:')\n",
    "print('Mean Squared Error (the lower the better)',mean_squared_error(y_test,y_pred))\n",
    "print('Root Mean Squared Error (average error per point)',np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets plot our line.\n",
    "M = linear.coef_ #This is how we extract our parameters (M, the gradient of the fit line).\n",
    "c = linear.intercept_ #This is how we extract our parameters (c, the y_intercept of the  fit line).\n",
    "plt.plot(X_test,y_test,'o')\n",
    "plt.plot(X_test,M*X_test+c,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets plot it has a nice table\n",
    "\n",
    "pred_data = pd.DataFrame(list(zip(y_test,y_pred,y_test-y_pred)),index=y_test.index,columns=['ground-truth','prediction','difference'])\n",
    "pred_data.head()\n",
    "#Notice the diff. This is our residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets plot the residuals\n",
    "plt.plot(X_test,pred_data['difference'],'o')\n",
    "#plt.plot(np.arange(0,pred_data.shape[0]),[0]*pred_data.shape[0],'--')\n",
    "plt.axhline(y=0)\n",
    "plt.title(\"Residuals\")\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7jIn9fwgMew"
   },
   "source": [
    "### 4. Non-linear data.\n",
    "In this exercise we try and fit a straight-line to some data which is not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AP52AGypxJW5"
   },
   "outputs": [],
   "source": [
    "#Here I just make up some data.\n",
    "X_nonlin = np.arange(0,100)\n",
    "y_nonlin = (X_nonlin*0.4 +(X_nonlin**2)*0.08 + 5)+(np.random.normal(size=100)-0.5)*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the data.\n",
    "plt.plot(X_nonlin, y_nonlin,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('has a high degree of correlation',np.corrcoef(X_nonlin,y_nonlin)[1][0])\n",
    "#This data is still highly correlated.\n",
    "#TODO:\n",
    "#Exercise. Fit this data with a linear regression model and then calculate the residuals.\n",
    "#How do the residuals look? Can you see evidence of the non-linearity in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Review\n",
    "* We have covered:\n",
    "  * Scaling and Standardizing Data \n",
    "  * Regression machine learning algorithms\n",
    "  * Correlation of data\n",
    "  * Residuals\n",
    "  * non-linear data.\n",
    "* We have of course not covered everything that it is possible to cover, and there is plenty more to research. \n",
    "* If you want to do more, you can do the advanced linear regression notebook  or....\n",
    "* If you are really serious about doing image analysis with Python please check this site: https://github.com/IAFIG-RMS/Python-for-Bioimage-Analysis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w0d_iE883eXP"
   ],
   "include_colab_link": true,
   "name": "Oxford_scikit_learn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
